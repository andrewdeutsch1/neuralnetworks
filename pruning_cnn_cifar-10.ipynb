{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning a CNN for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\r\n",
    "import tensorflow_model_optimization as tfmot\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from datetime import datetime as date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain dataset and display size\r\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\n",
    "(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\r\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\r\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize training data\r\n",
    "x_train_R = x_train.transpose()[0].transpose()\r\n",
    "x_train_G = x_train.transpose()[1].transpose()\r\n",
    "x_train_B = x_train.transpose()[2].transpose()\r\n",
    "\r\n",
    "# norm_vals = np.array([[0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]])\r\n",
    "\r\n",
    "def norm(arr):\r\n",
    "    return (arr - np.mean(arr)) / np.std(arr)\r\n",
    "\r\n",
    "x_train_R = norm(x_train_R)\r\n",
    "x_train_G = norm(x_train_G)\r\n",
    "x_train_B = norm(x_train_B)\r\n",
    "\r\n",
    "x_train = np.array([x_train_R, x_train_G, x_train_B])\r\n",
    "x_train = np.moveaxis(x_train, 0, -1)\r\n",
    "\r\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize test data\r\n",
    "x_test_R = x_test.transpose()[0].transpose()\r\n",
    "x_test_G = x_test.transpose()[1].transpose()\r\n",
    "x_test_B = x_test.transpose()[2].transpose()\r\n",
    "\r\n",
    "x_test_R = norm(x_test_R)\r\n",
    "x_test_G = norm(x_test_G)\r\n",
    "x_test_B = norm(x_test_B)\r\n",
    "\r\n",
    "x_test = np.array([x_test_R, x_test_G, x_test_B])\r\n",
    "x_test = np.moveaxis(x_test, 0, -1)\r\n",
    "\r\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 5.4830214442821065e-17\n",
      "StDev: 1.0000000000000002\n",
      "Mean: 8.14903700074865e-18\n",
      "StDev: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# verify normalization\r\n",
    "print('Mean:', np.mean(x_train))\r\n",
    "print('StDev:', np.std(x_train))\r\n",
    "\r\n",
    "print('Mean:', np.mean(x_test))\r\n",
    "print('StDev:', np.std(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               691500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 713,902\n",
      "Trainable params: 713,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build architecture, compile, display summary\r\n",
    "cnn = tf.keras.models.Sequential([\r\n",
    "    # preprocessing\r\n",
    "    tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),\r\n",
    "    # tf.keras.layers.experimental.preprocessing.RandomFlip(),\r\n",
    "    # tf.keras.layers.experimental.preprocessing.RandomCrop(28, 28),\r\n",
    "    # convolution and pooling\r\n",
    "    tf.keras.layers.Conv2D(filters=32, activation='relu', kernel_size=(3,3)),\r\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\r\n",
    "    tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(3,3)),\r\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\r\n",
    "    # dense\r\n",
    "    tf.keras.layers.Flatten(),\r\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\r\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\r\n",
    "])\r\n",
    "\r\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
    "\r\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/andre/neural/log/2021-06-23/11-44-49\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 13s 7ms/step - loss: 1.5102 - accuracy: 0.4564\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9029 - accuracy: 0.6815\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7066 - accuracy: 0.7531\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5430 - accuracy: 0.8137\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3980 - accuracy: 0.8620\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2770 - accuracy: 0.9030\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.1864 - accuracy: 0.9362\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1404 - accuracy: 0.9537\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1050 - accuracy: 0.9641\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0906 - accuracy: 0.9691\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0791 - accuracy: 0.9719\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.0712 - accuracy: 0.9763\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0729 - accuracy: 0.9740\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0694 - accuracy: 0.9769\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0621 - accuracy: 0.9792\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0638 - accuracy: 0.9782\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0563 - accuracy: 0.9818\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0599 - accuracy: 0.9804\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0516 - accuracy: 0.9833\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0593 - accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "# setup callbacks\r\n",
    "log_dir = \"C:/Users/andre/neural/log/\" + date.now().strftime(\"%Y-%m-%d/\") + date.now().strftime(\"%H-%M-%S\")\r\n",
    "print(log_dir)\r\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=1)\r\n",
    "\r\n",
    "# train\r\n",
    "training = cnn.fit(x_train, y_train, epochs=20, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 2.4617 - accuracy: 0.7124\n"
     ]
    }
   ],
   "source": [
    "# evaluate\r\n",
    "_, test_accuracy = cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_conv2d_2 (None, 30, 30, 32)        1762      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 15, 15, 32)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_3 (None, 13, 13, 64)        36930     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 6, 6, 64)          1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_flatten_ (None, 2304)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_2  (None, 300)               1382702   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_3  (None, 10)                6012      \n",
      "=================================================================\n",
      "Total params: 1,427,409\n",
      "Trainable params: 713,902\n",
      "Non-trainable params: 713,507\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# create model for pruning (all layers will be pruned)\r\n",
    "cnn_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(cnn)\r\n",
    "\r\n",
    "# find a way to print out the sparsity schedule\r\n",
    "\r\n",
    "# check to make sure layers are set for pruning\r\n",
    "cnn_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/andre/neural/log/2021-06-23/17-02-04\n",
      "Epoch 1/5\n",
      "   6/1563 [..............................] - ETA: 2:03 - loss: 0.7522 - accuracy: 0.6937WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0174s vs `on_train_batch_end` time: 0.0517s). Check your callbacks.\n",
      "1563/1563 [==============================] - 36s 22ms/step - loss: 0.6482 - accuracy: 0.7731\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.5139 - accuracy: 0.8242\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 61s 39ms/step - loss: 0.4242 - accuracy: 0.8551\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.3382 - accuracy: 0.8837\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 38s 25ms/step - loss: 0.2624 - accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "# log directory\r\n",
    "log_dir = \"C:/Users/andre/neural/log/\" + date.now().strftime(\"%Y-%m-%d/\") + date.now().strftime(\"%H-%M-%S\")\r\n",
    "print(log_dir)\r\n",
    "\r\n",
    "# special callback to log pruning data\r\n",
    "callbacks = [\r\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=1),\r\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\r\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir, update_freq=1)\r\n",
    "]\r\n",
    "\r\n",
    "cnn_for_pruning.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "pruned_training = cnn_for_pruning.fit(x_train, y_train, callbacks=callbacks, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 2.3411 - accuracy: 0.7007\n"
     ]
    }
   ],
   "source": [
    "# evaluate\r\n",
    "_, test_accuracy = cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning only some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_2  (None, 300)               1382702   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_3  (None, 10)                6012      \n",
      "=================================================================\n",
      "Total params: 1,408,106\n",
      "Trainable params: 713,902\n",
      "Non-trainable params: 694,204\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# designate only dense layers to be pruned\r\n",
    "def prune_dense(layer):\r\n",
    "    if isinstance(layer, tf.keras.layers.Dense):\r\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer)\r\n",
    "    return layer\r\n",
    "\r\n",
    "# clone the model and apply this function to each layer\r\n",
    "cnn_for_dense_pruning = tf.keras.models.clone_model(cnn, clone_function=prune_dense)\r\n",
    "\r\n",
    "# check to make sure only dense layers are affected\r\n",
    "cnn_for_dense_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN with dense pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/andre/neural/log/2021-06-23/14-36-45\n",
      "Epoch 1/20\n",
      "   3/1563 [..............................] - ETA: 2:11 - loss: 2.4029 - accuracy: 0.0799 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.\n",
      "1563/1563 [==============================] - 17s 10ms/step - loss: 1.4967 - accuracy: 0.4657\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.9273 - accuracy: 0.6799\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7575 - accuracy: 0.7350\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6105 - accuracy: 0.7886\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.5017 - accuracy: 0.8286\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3920 - accuracy: 0.8632\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3041 - accuracy: 0.8963\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2270 - accuracy: 0.9230\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1703 - accuracy: 0.9409\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.1371 - accuracy: 0.9538\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.1021 - accuracy: 0.9662\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.0914 - accuracy: 0.9697\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.0774 - accuracy: 0.9741\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0716 - accuracy: 0.9746\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0682 - accuracy: 0.9764\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0615 - accuracy: 0.9794\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0673 - accuracy: 0.9778\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0529 - accuracy: 0.9821\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0580 - accuracy: 0.9805\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0507 - accuracy: 0.9835\n"
     ]
    }
   ],
   "source": [
    "# log directory\r\n",
    "log_dir = \"C:/Users/andre/neural/log/\" + date.now().strftime(\"%Y-%m-%d/\") + date.now().strftime(\"%H-%M-%S\")\r\n",
    "print(log_dir)\r\n",
    "\r\n",
    "# special callback to log pruning data\r\n",
    "callbacks = [\r\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=1),\r\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\r\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir, update_freq=1)\r\n",
    "]\r\n",
    "\r\n",
    "cnn_for_dense_pruning.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "dense_pruned_training = cnn_for_dense_pruning.fit(x_train, y_train, callbacks=callbacks, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 2.3015 - accuracy: 0.7020\n"
     ]
    }
   ],
   "source": [
    "# evaluate\r\n",
    "_, test_accuracy = cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875408"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are mainly interested in the size of weights files\r\n",
    "cnn.save_weights(\"cnn_weights.h5\")\r\n",
    "os.path.getsize(\"cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5742908"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_for_pruning.save_weights(\"cnn_for_pruning_weights.h5\")\r\n",
    "os.path.getsize(\"cnn_for_pruning_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_weights\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.save(\"cnn_weights\")\r\n",
    "os.path.getsize(\"cnn_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11480268"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_for_pruning.save(\"cnn_for_pruning_weights.h5\")\r\n",
    "os.path.getsize(\"cnn_for_pruning_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gzipped model: 1656946.00 bytes\n",
      "Size of gzipped pruned model: 1838780.00 bytes\n",
      "Size of gzipped model: 1656946.00 bytes\n",
      "Size of gzipped pruned model: 1656946.00 bytes\n"
     ]
    }
   ],
   "source": [
    "import tempfile\r\n",
    "\r\n",
    "def get_gzipped_model_size(model):\r\n",
    "    import os\r\n",
    "    import zipfile\r\n",
    "\r\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\r\n",
    "    \r\n",
    "    # print(os.path.getsize(keras_file))\r\n",
    "    model.save(keras_file, include_optimizer=False)\r\n",
    "\r\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\r\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\r\n",
    "        f.write(keras_file)\r\n",
    "    \r\n",
    "    return os.path.getsize(zipped_file)\r\n",
    "\r\n",
    "# get_gzipped_model_size(cnn) # 2.6 MB\r\n",
    "\r\n",
    "# strip_pruning is necessary to see the compression benefits of pruning\r\n",
    "z_cnn = tfmot.sparsity.keras.strip_pruning(cnn)\r\n",
    "\r\n",
    "#get_gzipped_model_size(z_cnn)\r\n",
    "\r\n",
    "z_cnn_for_pruning = tfmot.sparsity.keras.strip_pruning(cnn_for_pruning)\r\n",
    "\r\n",
    "\r\n",
    "# z_cnn_for_dense_pruning = tfmot.sparsity.keras.strip_pruning(cnn_for_dense_pruning)\r\n",
    "print(\"Size of gzipped model: %.2f bytes\" % (get_gzipped_model_size(cnn)))\r\n",
    "print(\"Size of gzipped pruned model: %.2f bytes\" % (get_gzipped_model_size(cnn_for_pruning)))\r\n",
    "\r\n",
    "print(\"Size of gzipped model: %.2f bytes\" % (get_gzipped_model_size(z_cnn)))\r\n",
    "print(\"Size of gzipped pruned model: %.2f bytes\" % (get_gzipped_model_size(z_cnn_for_pruning)))\r\n",
    "# print(\"Size of gzipped dense pruned model: %.2f bytes\" % (get_gzipped_model_size(cnn_for_dense_pruning)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment with the effect that sparsity has (for loop of 0.1) on the accuracy of pruned networks\r\n",
    "quantization\r\n",
    "research how the models are being compressed\r\n",
    "research save_weights()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26130839a7e87fd59e038942810dbb4909e5232b32b1ad521e6a6bc3043702c6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}